# -*- coding: utf-8 -*-
"""BBC_Analyze.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MARVnW0imGr7REt5K15Wx36iGA1EFTvJ

**Prerequisite**

1. upload stopwords.txt and bbc_news.csv file
2. create folder name "table"
"""

!apt-get -qq update > /tmp/apt.out
!apt-get install -y -qq openjdk-11-jdk-headless

!(wget -q --show-progress -nc https://mirrors.ocf.berkeley.edu/apache/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz)
!tar xf spark-3.2.3-bin-hadoop3.2.tgz

# Commented out IPython magic to ensure Python compatibility.
try:
  import pyspark, findspark, delta
except:
#   %pip install -q --upgrade pyspark==3.2.1
#   %pip install -q findspark
#   %pip install -q delta

import findspark
import pyspark
import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.3-bin-hadoop3.2"

findspark.init()
MAX_MEMORY="8g"
maven_coords = [
    "org.apache.spark:spark-avro_2.12:3.2.1",
    "io.delta:delta-core_2.12:2.0.0rc1",
    "org.xerial:sqlite-jdbc:3.36.0.3",
    "graphframes:graphframes:0.8.2-spark3.2-s_2.12",
    "com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.8",
]

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from pyspark.sql.functions import split, explode, length, col, regexp_extract, count, row_number, desc, collect_list
from pyspark.sql.window import Window

spark = (pyspark.sql.SparkSession.builder.appName("MyApp") 
    .config("spark.jars.packages", ",".join(maven_coords))
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") 
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") 
    .config("spark.executor.memory", MAX_MEMORY) 
    .config("spark.driver.memory", MAX_MEMORY) 
    .enableHiveSupport() 
    .getOrCreate()        
    )

spark

bcc_file_path="/content/bbc_news.csv"
stopwords_file_path = "/content/stopwords.txt"


bbc_schema = StructType([
    StructField("ArticleID", IntegerType(), True),
    StructField("Article", StringType(), True),
    StructField("Category", StringType(), True)
])
bbc_df = spark.read.format("csv").option("header", "true").schema(bbc_schema).load(bcc_file_path)

data_rdd = spark.sparkContext.textFile(stopwords_file_path)
data_list = data_rdd.collect()

w = Window.partitionBy("ArticleID").orderBy(desc("Frequency"))

bbc_df = bbc_df.selectExpr("ArticleID", "explode(split(Article, '[ ,:;!\\?]+')) as Tag","Category") \
                 .filter((length(col("Tag")) > 2) & (~col("Tag").isin(data_list)) & (regexp_extract(col("Tag"), r"\b\w*\d+\w*\b", 0) == "")) \
                 .groupBy("ArticleID", "Tag", "Category").agg(count("*").alias("Frequency")) \
                 .filter(col("Frequency") >= 10) \
                 .select("*", row_number().over(w).alias("row_number")).filter(col("row_number") <= 3).drop("row_number")

bbc_df.show(10)

bbc_df.write.mode("overwrite").format("parquet").option("compression", "snappy").partitionBy("Category").saveAsTable("bbc_data", path="/content/table")

"""Data can be seen in table folder"""

read_df = spark.sql("Select * from bbc_data")
read_df.show(10)